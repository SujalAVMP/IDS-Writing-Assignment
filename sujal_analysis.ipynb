{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 328 Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib as mpl\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read WID CSV files with proper parameters\n",
    "def read_wid_csv(file_path):\n",
    "    \"\"\"\n",
    "    Read WID CSV files using the semicolon separator as specified in documentation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, sep=';', encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to list available files in the WID data directory\n",
    "def list_wid_files(directory='wid_all_data'):\n",
    "    \"\"\"List and categorize files in the WID data directory.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory {directory} not found\")\n",
    "        return None\n",
    "    \n",
    "    all_files = os.listdir(directory)\n",
    "    \n",
    "    # Categorize files\n",
    "    country_file = [f for f in all_files if f == 'WID_countries.csv']\n",
    "    data_files = sorted([f for f in all_files if f.startswith('WID_data_')])\n",
    "    metadata_files = sorted([f for f in all_files if f.startswith('WID_metadata_')])\n",
    "    other_files = [f for f in all_files if f not in country_file + data_files + metadata_files]\n",
    "    \n",
    "    # Create a summary dictionary\n",
    "    file_summary = {\n",
    "        'country_file': country_file,\n",
    "        'data_files': data_files,\n",
    "        'metadata_files': metadata_files,\n",
    "        'other_files': other_files,\n",
    "        'total_files': len(all_files),\n",
    "        'total_countries': len(data_files)\n",
    "    }\n",
    "    \n",
    "    return file_summary\n",
    "\n",
    "# Explore available countries and their metadata\n",
    "def explore_countries(directory='wid_all_data'):\n",
    "    \"\"\"Load and explore country data from WID_countries.csv.\n",
    "       Returns a summary of regions and countries.    \n",
    "    \"\"\"\n",
    "    countries_path = os.path.join(directory, 'WID_countries.csv')\n",
    "    \n",
    "    if not os.path.exists(countries_path):\n",
    "        print(f\"Country file not found at {countries_path}\")\n",
    "        return None\n",
    "    \n",
    "    countries_df = read_wid_csv(countries_path)\n",
    "    \n",
    "    if countries_df is not None:\n",
    "        # Create a summary of regions\n",
    "        region_counts = countries_df['region'].value_counts()\n",
    "        region2_counts = countries_df['region2'].value_counts()\n",
    "        \n",
    "        # Filter actual countries (2-letter codes) from regions/aggregates\n",
    "        countries_only = countries_df[countries_df['alpha2'].str.len() == 2]\n",
    "        \n",
    "        # Create a country summary\n",
    "        country_summary = {\n",
    "            'total_entries': len(countries_df),\n",
    "            'country_count': len(countries_only),\n",
    "            'regions': region_counts.to_dict(),\n",
    "            'subregions': region2_counts.to_dict()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'countries_df': countries_df,\n",
    "            'summary': country_summary\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to explore the structure of a single country's data file\n",
    "def explore_country_data(country_code, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Explore the data structure for a single country.\n",
    "    \n",
    "    Args:\n",
    "        country_code (str): Two-letter country code (e.g., 'US', 'FR')\n",
    "        directory (str): Path to the WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary information about the country's data\n",
    "        Contains data_df, metadata_df, data_summary, and metadata_summary\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(directory, f'WID_data_{country_code}.csv')\n",
    "    metadata_path = os.path.join(directory, f'WID_metadata_{country_code}.csv')\n",
    "    \n",
    "    if not os.path.exists(data_path) or not os.path.exists(metadata_path):\n",
    "        print(f\"Data or metadata file for {country_code} not found\")\n",
    "        return None\n",
    "    \n",
    "    # Load data and metadata\n",
    "    data_df = read_wid_csv(data_path)\n",
    "    metadata_df = read_wid_csv(metadata_path)\n",
    "    \n",
    "    if data_df is None or metadata_df is None:\n",
    "        return None\n",
    "    \n",
    "    # Create data summary\n",
    "    data_summary = {\n",
    "        'rows': len(data_df),\n",
    "        'variables': data_df['variable'].nunique(),\n",
    "        'variable_list': sorted(data_df['variable'].unique()),\n",
    "        'percentiles': data_df['percentile'].nunique(),\n",
    "        'percentile_list': sorted(data_df['percentile'].unique()),\n",
    "        'years': {\n",
    "            'min': data_df['year'].min(),\n",
    "            'max': data_df['year'].max(),\n",
    "            'count': data_df['year'].nunique()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create metadata summary\n",
    "    metadata_summary = {\n",
    "        'rows': len(metadata_df),\n",
    "        'unique_variables': metadata_df['variable'].nunique(),\n",
    "        'variable_list': sorted(metadata_df['variable'].unique())\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'data_df': data_df,\n",
    "        'metadata_df': metadata_df,\n",
    "        'data_summary': data_summary,\n",
    "        'metadata_summary': metadata_summary\n",
    "    }\n",
    "\n",
    "# Function to extract variable descriptions from metadata\n",
    "def get_variable_descriptions(metadata_df):\n",
    "    \"\"\"\n",
    "    Extract unique variable descriptions from metadata.\n",
    "    \n",
    "    Args:\n",
    "        metadata_df (pd.DataFrame): Metadata dataframe\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with variable codes and descriptions\n",
    "    \"\"\"\n",
    "    if metadata_df is None:\n",
    "        return None\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    required_columns = ['variable', 'age', 'pop', 'shortname', 'simpledes', 'technicaldes', 'longtype', 'shortpop', 'longpop', 'shortage', 'longage', 'unit']\n",
    "    if not all(col in metadata_df.columns for col in required_columns):\n",
    "        print(f\"Metadata is missing required columns. Available columns: {metadata_df.columns.tolist()}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract unique variable descriptions\n",
    "    var_descriptions = metadata_df[required_columns].drop_duplicates()\n",
    "    print(f\"Variable Descriptions DataFrame:\\n{var_descriptions.sort_values('variable').reset_index(drop=True).head()}\")\n",
    "    return var_descriptions.sort_values('variable').reset_index(drop=True)\n",
    "\n",
    "# Function to examine variable availability across countries\n",
    "def compare_variable_availability(country_list, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Compare which variables are available across multiple countries.\n",
    "    \n",
    "    Args:\n",
    "        country_list (list): List of country codes to compare\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame showing variable availability by country\n",
    "        pd.DataFrame: Pivot table of variable availability\n",
    "    \"\"\"\n",
    "    availability_data = []\n",
    "    \n",
    "    for country in country_list:\n",
    "        data_path = os.path.join(directory, f'WID_data_{country}.csv')\n",
    "        \n",
    "        if os.path.exists(data_path):\n",
    "            data_df = read_wid_csv(data_path)\n",
    "            \n",
    "            if data_df is not None:\n",
    "                variables = data_df['variable'].unique()\n",
    "                \n",
    "                for var in variables:\n",
    "                    # Check year range for this variable\n",
    "                    var_data = data_df[data_df['variable'] == var]\n",
    "                    year_min = var_data['year'].min()\n",
    "                    year_max = var_data['year'].max()\n",
    "                    \n",
    "                    availability_data.append({\n",
    "                        'country': country,\n",
    "                        'variable': var,\n",
    "                        'available': True,\n",
    "                        'year_min': year_min,\n",
    "                        'year_max': year_max,\n",
    "                        'year_count': var_data['year'].nunique()\n",
    "                    })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    availability_df = pd.DataFrame(availability_data)\n",
    "    \n",
    "    # print(f\"Availability DataFrame:\\n{availability_df.head()}\")\n",
    "    # Create a pivot table of availability\n",
    "    if not availability_df.empty:\n",
    "        pivot_df = pd.pivot_table(\n",
    "            availability_df, \n",
    "            values='available',\n",
    "            index='variable',\n",
    "            columns='country',\n",
    "            aggfunc=lambda x: True if len(x) > 0 else False,\n",
    "            fill_value=False\n",
    "        )\n",
    "        \n",
    "        # print(f\"Pivot Table:\\n{pivot_df.head()}\")\n",
    "        # Add a total count column\n",
    "        pivot_df['total_countries'] = pivot_df.sum(axis=1)\n",
    "        \n",
    "        # Sort by availability\n",
    "        pivot_df = pivot_df.sort_values('total_countries', ascending=False)\n",
    "        \n",
    "        return availability_df, pivot_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "# Main execution to explore the dataset\n",
    "def explore_dataset(directory='wid_all_data'):\n",
    "    \"\"\"Main function to explore the WID dataset structure.\"\"\"\n",
    "    print(\"Exploring WID dataset structure...\")\n",
    "    \n",
    "    # List available files\n",
    "    files = list_wid_files(directory)\n",
    "    if files:\n",
    "        print(f\"Total files: {files['total_files']}\")\n",
    "        print(f\"Country files: {len(files['data_files'])}\")\n",
    "        \n",
    "        # Show some example countries\n",
    "        if files['data_files']:\n",
    "            print(\"Example countries:\", [f.replace('WID_data_', '').replace('.csv', '') \n",
    "                                         for f in files['data_files'][:10]])\n",
    "    \n",
    "    # Explore countries metadata\n",
    "    countries_info = explore_countries(directory)\n",
    "    if countries_info:\n",
    "        countries_df = countries_info['countries_df']\n",
    "        print(f\"\\nTotal countries/regions: {len(countries_df)}\")\n",
    "        \n",
    "        # Display regions\n",
    "        print(\"\\nWorld regions:\")\n",
    "        for region, count in countries_info['summary']['regions'].items():\n",
    "            print(f\"  {region}: {count} entries\")\n",
    "    \n",
    "    # Explore a sample country\n",
    "    sample_country = 'US'  # United States as example\n",
    "    country_info = explore_country_data(sample_country, directory)\n",
    "    \n",
    "    if country_info:\n",
    "        print(f\"\\nSample data for {sample_country}:\")\n",
    "        # print(f\"  Data Summary: {country_info['data_summary']}\")\n",
    "        # print(f\"  Metadata Summary: {country_info['metadata_summary']}\")\n",
    "        print(f\"  Rows: {country_info['data_summary']['rows']}\")\n",
    "        print(f\"  Unique variables: {country_info['data_summary']['variables']}\")\n",
    "        print(f\"  Year range: {country_info['data_summary']['years']['min']} - {country_info['data_summary']['years']['max']}\")\n",
    "        \n",
    "        # Show some variable descriptions\n",
    "        var_desc = get_variable_descriptions(country_info['metadata_df'])\n",
    "        if var_desc is not None and len(var_desc) > 0:\n",
    "            print(\"\\nSample variable descriptions:\")\n",
    "            for _, row in var_desc.head(5).iterrows():\n",
    "                print(f\"  {row['variable']}: {row['longtype']} ({row['unit']})\")\n",
    "    \n",
    "    return {\n",
    "        'files': files,\n",
    "        'countries_info': countries_info,\n",
    "        'sample_country_info': country_info\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key variables we'll be using based on what's available for all countries\n",
    "# These variables are confirmed to exist for all countries in our analysis\n",
    "\n",
    "# Income variables\n",
    "INCOME_SHARE_VARIABLES = ['sptincj992']  # Share of pre-tax national income with equal-split adults\n",
    "INCOME_GINI_VARIABLES = ['gptincj992']   # Gini coefficient for pre-tax income\n",
    "INCOME_AVERAGE_VARIABLES = ['aptincj992', 'bptincj992']  # Average metrics for pre-tax income\n",
    "\n",
    "# anninci992 anninci999 - nninc net national income\n",
    "\n",
    "# Wealth variables\n",
    "WEALTH_SHARE_VARIABLES = ['shwealj992']  # Share of household wealth with equal-split adults\n",
    "WEALTH_GINI_VARIABLES = ['ghwealj992']   # Gini coefficient for household wealth\n",
    "WEALTH_AVERAGE_VARIABLES = ['bhwealj992', 'ahwealj992']  # Average metrics for household wealth\n",
    "\n",
    "# Combined lists for easier processing\n",
    "SHARE_VARIABLES = INCOME_SHARE_VARIABLES + WEALTH_SHARE_VARIABLES\n",
    "GINI_VARIABLES = INCOME_GINI_VARIABLES + WEALTH_GINI_VARIABLES\n",
    "AVERAGE_VARIABLES = INCOME_AVERAGE_VARIABLES + WEALTH_AVERAGE_VARIABLES\n",
    "\n",
    "# Define combined variables for use in functions that expect INCOME_VARIABLES and WEALTH_VARIABLES\n",
    "INCOME_VARIABLES = INCOME_SHARE_VARIABLES + INCOME_GINI_VARIABLES + INCOME_AVERAGE_VARIABLES\n",
    "WEALTH_VARIABLES = WEALTH_SHARE_VARIABLES + WEALTH_GINI_VARIABLES + WEALTH_AVERAGE_VARIABLES\n",
    "\n",
    "# Define percentiles of interest\n",
    "TOP_PERCENTILES = ['p99p100', 'p90p100']  # Top 1%, Top 10%\n",
    "BOTTOM_PERCENTILES = ['p0p50']  # Bottom 50%\n",
    "MIDDLE_PERCENTILES = ['p50p90']  # Middle 40%\n",
    "\n",
    "# Countries to include in our analysis\n",
    "# We'll select a diverse set of countries from different regions and development levels\n",
    "COUNTRIES_TO_ANALYZE = [\n",
    "    # High-income countries\n",
    "    'US',   # United States\n",
    "    'FR',   # France\n",
    "    'DE',   # Germany\n",
    "    'GB',   # United Kingdom\n",
    "    'JP',   # Japan\n",
    "    \n",
    "    # Upper-middle income countries\n",
    "    'BR',   # Brazil\n",
    "    'CN',   # China\n",
    "    'RU',   # Russia\n",
    "    'ZA',   # South Africa\n",
    "    \n",
    "    # Lower-middle and low-income countries\n",
    "    'IN',   # India\n",
    "    'ID',   # Indonesia\n",
    "    'NG',   # Nigeria\n",
    "    'EG'    # Egypt\n",
    "]\n",
    "\n",
    "# Function to load country data with selected variables\n",
    "def load_country_data(country_code, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Load specific inequality variables for a given country.\n",
    "    \n",
    "    Args:\n",
    "        country_code (str): Two-letter country code\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (data_df, metadata_df) for the country\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(directory, f'WID_data_{country_code}.csv')\n",
    "    metadata_path = os.path.join(directory, f'WID_metadata_{country_code}.csv')\n",
    "    \n",
    "    if not os.path.exists(data_path) or not os.path.exists(metadata_path):\n",
    "        print(f\"Data or metadata not found for {country_code}\")\n",
    "        return None, None\n",
    "    \n",
    "    data_df = read_wid_csv(data_path)\n",
    "    metadata_df = read_wid_csv(metadata_path)\n",
    "    \n",
    "    return data_df, metadata_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a dataset for a specific inequality metric\n",
    "def create_inequality_dataset(countries, variable_codes, percentiles, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Create a dataset comparing specific inequality variables across countries.\n",
    "    Will try each variable code in the list until one works.\n",
    "    \n",
    "    Args:\n",
    "        countries (list): List of country codes\n",
    "        variable_codes (list or str): WID variable code(s) to try\n",
    "        percentiles (list): List of percentile codes (e.g., ['p99p100', 'p0p50'])\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset with inequality data\n",
    "    \"\"\"\n",
    "    # Convert single variable code to list for consistent processing\n",
    "    if isinstance(variable_codes, str):\n",
    "        variable_codes = [variable_codes]    \n",
    "    # Load country information for names\n",
    "    countries_info = explore_countries(directory)\n",
    "    countries_df = countries_info['countries_df']\n",
    "    country_name_map = dict(zip(countries_df['alpha2'], countries_df['shortname']))\n",
    "    \n",
    "    # Try each variable code until we find one that works\n",
    "    for variable_code in variable_codes:\n",
    "        print(f\"Trying variable code: {variable_code}\")\n",
    "        combined_df = pd.DataFrame()\n",
    "        \n",
    "        for country in countries:\n",
    "            data_df, metadata_df = load_country_data(country, directory)\n",
    "            \n",
    "            if data_df is None:\n",
    "                print(f\"  Skipping {country} - could not load data\")\n",
    "                continue\n",
    "            \n",
    "            # Filter for the requested variable and percentiles\n",
    "            filtered_df = data_df[(data_df['variable'] == variable_code) & \n",
    "                                (data_df['percentile'].isin(percentiles))]\n",
    "            \n",
    "            if filtered_df.empty:\n",
    "                print(f\"  No data for {variable_code} with percentiles {percentiles} in {country}\")\n",
    "                continue\n",
    "            \n",
    "            # Add country name\n",
    "            filtered_df['country_code'] = country\n",
    "            filtered_df['country_name'] = country_name_map.get(country, country)\n",
    "            \n",
    "            # Append to combined dataset\n",
    "            combined_df = pd.concat([combined_df, filtered_df])\n",
    "            print(f\"  Found data for {variable_code} for {country}: {len(filtered_df)} rows\")\n",
    "        \n",
    "        if not combined_df.empty:\n",
    "            print(f\"Successfully found data for variable {variable_code}\")\n",
    "            return combined_df\n",
    "    \n",
    "    print(f\"No data found for any of these variables: {variable_codes} across specified countries and percentiles\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to create a comparative dataset of income/wealth distribution over time\n",
    "def create_time_series_dataset(variable_codes, percentile, countries=COUNTRIES_TO_ANALYZE, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Create a dataset of inequality metrics over time for multiple countries.\n",
    "    Will try multiple variable codes until one works.\n",
    "    \n",
    "    Args:\n",
    "        variable_codes (list or str): WID variable code(s) to try\n",
    "        percentile (str): Percentile code\n",
    "        countries (list): List of country codes\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Time series data for the specified variable and percentile\n",
    "    \"\"\"\n",
    "    # Convert single variable code to list for consistent processing\n",
    "    if isinstance(variable_codes, str):\n",
    "        variable_codes = [variable_codes]\n",
    "    \n",
    "    # Get variable description (try to get from first variable code, but not critical)\n",
    "    variable_desc = None\n",
    "    try:\n",
    "        sample_country = countries[0]\n",
    "        _, metadata_df = load_country_data(sample_country, directory)\n",
    "        \n",
    "        if metadata_df is not None:\n",
    "            for var_code in variable_codes:\n",
    "                var_info = metadata_df[metadata_df['variable'] == var_code]\n",
    "                if not var_info.empty:\n",
    "                    variable_desc = var_info.iloc[0]['simpledes']\n",
    "                    break\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Try to create dataset with any of the provided variable codes\n",
    "    dataset = create_inequality_dataset(countries, variable_codes, [percentile], directory)\n",
    "    \n",
    "    if dataset is not None:\n",
    "        # Pivot to have years as columns and countries as rows for easier plotting\n",
    "        dataset = dataset.sort_values(['country_name', 'year'])\n",
    "        \n",
    "        # Add metadata\n",
    "        dataset.attrs['variable_code'] = dataset['variable'].iloc[0]  # Use the actual variable code that worked\n",
    "        dataset.attrs['variable_desc'] = variable_desc\n",
    "        dataset.attrs['percentile'] = percentile\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Function to create a dataset for GDP per capita\n",
    "def create_gdp_dataset(countries=COUNTRIES_TO_ANALYZE, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Create a dataset of GDP per capita for comparison with inequality metrics.\n",
    "    Using national income per adult as a proxy.\n",
    "    \n",
    "    Args:\n",
    "        countries (list): List of country codes\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: GDP per capita data\n",
    "    \"\"\"\n",
    "    # Try several possible GDP/income per adult variables\n",
    "    gdp_variables = ['anninc992i', 'aptinc992i', 'adiinci992', 'adiincf992']\n",
    "    \n",
    "    # We don't need a percentile for this aggregate measure, but WID still requires one\n",
    "    # p0p100 represents the entire population\n",
    "    gdp_data = create_inequality_dataset(countries, gdp_variables, ['p0p100'], directory)\n",
    "    \n",
    "    if gdp_data is not None:\n",
    "        # Add variable description\n",
    "        var_code = gdp_data['variable'].iloc[0]\n",
    "        gdp_data.attrs['variable_desc'] = f'Income per Adult ({var_code})'\n",
    "        \n",
    "        # Convert to common currency (USD) using most recent PPP rates\n",
    "        # This would require additional implementation to get PPP conversion rates\n",
    "        # For simplicity, we'll leave the values in local currency\n",
    "    \n",
    "    return gdp_data\n",
    "\n",
    "# Function to create a cross-sectional dataset with the available variables\n",
    "def create_cross_sectional_dataset(countries=COUNTRIES_TO_ANALYZE, year=2020, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Create a cross-sectional dataset combining multiple inequality metrics for a specific year.\n",
    "    Adapts to find available variables in the dataset.\n",
    "    \n",
    "    Args:\n",
    "        countries (list): List of country codes\n",
    "        year (int): Reference year for the cross-section\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset with multiple inequality metrics\n",
    "    \"\"\"\n",
    "    print(\"Building cross-sectional dataset with available metrics...\")\n",
    "    \n",
    "    # Initialize results dataframe with country codes\n",
    "    countries_info = explore_countries(directory)\n",
    "    country_name_map = {}\n",
    "    \n",
    "    if countries_info is not None and 'countries_df' in countries_info:\n",
    "        countries_df = countries_info['countries_df']\n",
    "        country_name_map = dict(zip(countries_df['alpha2'], countries_df['shortname']))\n",
    "    \n",
    "    # Create the base dataframe with country information\n",
    "    result_df = pd.DataFrame({\n",
    "        'country_code': countries,\n",
    "        'country_name': [country_name_map.get(c, c) for c in countries]\n",
    "    })\n",
    "    \n",
    "    # Identify what variables we have available from our time series datasets\n",
    "    available_datasets = []\n",
    "    \n",
    "    # Try loading income variables for different percentiles\n",
    "    for percentile in TOP_PERCENTILES + BOTTOM_PERCENTILES:\n",
    "        for var_prefix in ['a', 's']:  # Try both average and share variables\n",
    "            for var_type in INCOME_VARIABLES:\n",
    "                # Only use the variable type part (e.g., 'ptincf992' from 'aptincf992')\n",
    "                var_base = var_type[1:] if var_type.startswith('a') or var_type.startswith('s') else var_type\n",
    "                test_var = f\"{var_prefix}{var_base}\"\n",
    "                \n",
    "                dataset = create_inequality_dataset(countries, [test_var], [percentile], directory)\n",
    "                if dataset is not None and not dataset.empty:\n",
    "                    metric_name = f\"{var_prefix}_{var_base}_{percentile}\"\n",
    "                    available_datasets.append({\n",
    "                        'name': metric_name,\n",
    "                        'dataset': dataset,\n",
    "                        'variable': test_var,\n",
    "                        'percentile': percentile\n",
    "                    })\n",
    "                    print(f\"  Found data for {test_var} with percentile {percentile}\")\n",
    "    \n",
    "    # Try loading wealth variables\n",
    "    for percentile in TOP_PERCENTILES + BOTTOM_PERCENTILES:\n",
    "        for wealth_var in WEALTH_VARIABLES:\n",
    "            dataset = create_inequality_dataset(countries, [wealth_var], [percentile], directory)\n",
    "            if dataset is not None and not dataset.empty:\n",
    "                metric_name = f\"{wealth_var}_{percentile}\"\n",
    "                available_datasets.append({\n",
    "                    'name': metric_name,\n",
    "                    'dataset': dataset,\n",
    "                    'variable': wealth_var,\n",
    "                    'percentile': percentile\n",
    "                })\n",
    "                print(f\"  Found data for {wealth_var} with percentile {percentile}\")\n",
    "    \n",
    "    # Try loading GDP or income per adult variables\n",
    "    gdp_data = create_gdp_dataset(countries, directory)\n",
    "    if gdp_data is not None and not gdp_data.empty:\n",
    "        var_code = gdp_data['variable'].iloc[0]\n",
    "        available_datasets.append({\n",
    "            'name': f\"{var_code}_per_adult\",\n",
    "            'dataset': gdp_data,\n",
    "            'variable': var_code,\n",
    "            'percentile': 'p0p100'\n",
    "        })\n",
    "        print(f\"  Found GDP/income per adult data: {var_code}\")\n",
    "    \n",
    "    # Extract values for the reference year (or closest available)\n",
    "    for dataset_info in available_datasets:\n",
    "        df = dataset_info['dataset']\n",
    "        name = dataset_info['name']\n",
    "        \n",
    "        # Initialize new columns with NaN\n",
    "        result_df[f\"{name}_value\"] = np.nan\n",
    "        result_df[f\"{name}_year\"] = np.nan\n",
    "        \n",
    "        # Process each country\n",
    "        for country in result_df['country_code']:\n",
    "            country_data = df[df['country_code'] == country]\n",
    "            \n",
    "            if not country_data.empty:\n",
    "                # Try to get the exact year first\n",
    "                year_data = country_data[country_data['year'] == year]\n",
    "                \n",
    "                # If exact year not available, find closest year\n",
    "                if year_data.empty:\n",
    "                    available_years = country_data['year'].unique()\n",
    "                    if len(available_years) > 0:\n",
    "                        closest_year = available_years[np.abs(available_years - year).argmin()]\n",
    "                        year_data = country_data[country_data['year'] == closest_year]\n",
    "                \n",
    "                # If we found data, add it to the result\n",
    "                if not year_data.empty:\n",
    "                    row_idx = result_df[result_df['country_code'] == country].index[0]\n",
    "                    result_df.loc[row_idx, f\"{name}_value\"] = year_data['value'].iloc[0]\n",
    "                    result_df.loc[row_idx, f\"{name}_year\"] = year_data['year'].iloc[0]\n",
    "    \n",
    "    # Add region information if available\n",
    "    if countries_info is not None and 'countries_df' in countries_info:\n",
    "        countries_df = countries_info['countries_df']\n",
    "        \n",
    "        for idx, row in result_df.iterrows():\n",
    "            country_info = countries_df[countries_df['alpha2'] == row['country_code']]\n",
    "            if not country_info.empty:\n",
    "                result_df.loc[idx, 'region'] = country_info['region'].iloc[0]\n",
    "                result_df.loc[idx, 'region2'] = country_info['region2'].iloc[0]\n",
    "    \n",
    "    # Display summary of metrics found\n",
    "    value_cols = [col for col in result_df.columns if col.endswith('_value')]\n",
    "    print(f\"Created cross-sectional dataset with {len(value_cols)} metrics for {len(result_df)} countries\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Function to create a dataset comparing changes in inequality over time\n",
    "def create_inequality_change_dataset(countries=COUNTRIES_TO_ANALYZE, \n",
    "                                     variable_codes=INCOME_VARIABLES,\n",
    "                                     percentile='p99p100', \n",
    "                                     start_year=1980, \n",
    "                                     end_year=2020,\n",
    "                                     directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Create a dataset showing changes in inequality metrics over time.\n",
    "    \n",
    "    Args:\n",
    "        countries (list): List of country codes\n",
    "        variable_codes (list or str): WID variable code(s) to try\n",
    "        percentile (str): Percentile code\n",
    "        start_year (int): Starting year for change calculation\n",
    "        end_year (int): Ending year for change calculation\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with inequality changes\n",
    "    \"\"\"\n",
    "    # Get the time series data\n",
    "    time_series = create_time_series_dataset(variable_codes, percentile, countries, directory)\n",
    "    \n",
    "    if time_series is None:\n",
    "        return None\n",
    "    \n",
    "    # Calculate changes\n",
    "    change_data = []\n",
    "    \n",
    "    # Group by country\n",
    "    for country, group in time_series.groupby('country_code'):\n",
    "        group = group.sort_values('year')\n",
    "        country_name = group['country_name'].iloc[0]\n",
    "        \n",
    "        # Try to get values for exact years\n",
    "        start_data = group[group['year'] == start_year]\n",
    "        end_data = group[group['year'] == end_year]\n",
    "        \n",
    "        # If exact years not available, find closest years\n",
    "        if start_data.empty:\n",
    "            available_years = group['year'].unique()\n",
    "            closest_start = available_years[np.abs(available_years - start_year).argmin()]\n",
    "            start_data = group[group['year'] == closest_start]\n",
    "        \n",
    "        if end_data.empty:\n",
    "            available_years = group['year'].unique()\n",
    "            closest_end = available_years[np.abs(available_years - end_year).argmin()]\n",
    "            end_data = group[group['year'] == closest_end]\n",
    "        \n",
    "        # Skip if we don't have data for both periods\n",
    "        if start_data.empty or end_data.empty:\n",
    "            print(f\"Insufficient data for {country} to calculate changes\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate changes\n",
    "        start_value = start_data['value'].iloc[0]\n",
    "        end_value = end_data['value'].iloc[0]\n",
    "        actual_start_year = start_data['year'].iloc[0]\n",
    "        actual_end_year = end_data['year'].iloc[0]\n",
    "        \n",
    "        absolute_change = end_value - start_value\n",
    "        percent_change = (absolute_change / start_value) * 100 if start_value != 0 else np.nan\n",
    "        \n",
    "        change_data.append({\n",
    "            'country_code': country,\n",
    "            'country_name': country_name,\n",
    "            'start_year': actual_start_year,\n",
    "            'end_year': actual_end_year,\n",
    "            'start_value': start_value,\n",
    "            'end_value': end_value,\n",
    "            'absolute_change': absolute_change,\n",
    "            'percent_change': percent_change\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    change_df = pd.DataFrame(change_data)\n",
    "    \n",
    "    if change_df.empty:\n",
    "        return None\n",
    "    \n",
    "    # Add metadata\n",
    "    change_df.attrs['variable_code'] = time_series.attrs.get('variable_code', '')\n",
    "    change_df.attrs['variable_desc'] = time_series.attrs.get('variable_desc', '')\n",
    "    change_df.attrs['percentile'] = percentile\n",
    "    \n",
    "    return change_df\n",
    "\n",
    "# Function to combine income and wealth inequality data for correlation analysis\n",
    "def create_correlation_dataset(countries=COUNTRIES_TO_ANALYZE, reference_year=2020, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Create a dataset to analyze correlations between income and wealth inequality.\n",
    "    \n",
    "    Args:\n",
    "        countries (list): List of country codes\n",
    "        reference_year (int): Reference year for the cross-section\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with income and wealth inequality metrics\n",
    "    \"\"\"\n",
    "    # Get cross-sectional data\n",
    "    cross_section = create_cross_sectional_dataset(countries, reference_year, directory)\n",
    "    \n",
    "    if cross_section is None or cross_section.empty:\n",
    "        print(\"Could not create cross-sectional dataset for correlation analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Create metrics for correlation analysis\n",
    "    corr_metrics = [\n",
    "        ('top1_income_share', 'top1_wealth_share'),\n",
    "        ('top10_income_share', 'top10_wealth_share'),\n",
    "        ('bottom50_income_share', 'bottom50_wealth_share'),\n",
    "        ('gdp_per_adult', 'top1_income_share'),\n",
    "        ('gdp_per_adult', 'top1_wealth_share')\n",
    "    ]\n",
    "    \n",
    "    # Calculate correlations\n",
    "    correlations = {}\n",
    "    \n",
    "    for x_var, y_var in corr_metrics:\n",
    "        if x_var in cross_section.columns and y_var in cross_section.columns:\n",
    "            # Filter out NaN values\n",
    "            valid_data = cross_section[[x_var, y_var]].dropna()\n",
    "            \n",
    "            if len(valid_data) >= 5:  # Require at least 5 countries for meaningful correlation\n",
    "                corr, p_value = stats.pearsonr(valid_data[x_var], valid_data[y_var])\n",
    "                correlations[f'{x_var}_vs_{y_var}'] = {\n",
    "                    'correlation': corr,\n",
    "                    'p_value': p_value,\n",
    "                    'n': len(valid_data)\n",
    "                }\n",
    "    \n",
    "    # Add correlations to dataset attributes\n",
    "    cross_section.attrs['correlations'] = correlations\n",
    "    \n",
    "    return cross_section\n",
    "\n",
    "# Main function to prepare all datasets\n",
    "def prepare_all_datasets(directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Prepare all datasets needed for our inequality analysis with a focus on\n",
    "    comparing wealth and income inequality patterns across countries.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of prepared datasets\n",
    "    \"\"\"\n",
    "    print(\"Preparing inequality datasets...\")\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    # 1. Income share datasets for different percentiles\n",
    "    for percentile in TOP_PERCENTILES + BOTTOM_PERCENTILES:\n",
    "        print(f\"Creating income share dataset for {percentile}...\")\n",
    "        datasets[f'income_share_{percentile}'] = create_time_series_dataset(\n",
    "            INCOME_SHARE_VARIABLES, percentile, COUNTRIES_TO_ANALYZE, directory)\n",
    "    \n",
    "    # 2. Wealth share datasets for different percentiles\n",
    "    for percentile in TOP_PERCENTILES + BOTTOM_PERCENTILES:\n",
    "        print(f\"Creating wealth share dataset for {percentile}...\")\n",
    "        datasets[f'wealth_share_{percentile}'] = create_time_series_dataset(\n",
    "            WEALTH_SHARE_VARIABLES, percentile, COUNTRIES_TO_ANALYZE, directory)\n",
    "    \n",
    "    # 3. Income and wealth Gini coefficients (overall inequality metrics)\n",
    "    print(\"Creating income Gini coefficient dataset...\")\n",
    "    datasets['income_gini'] = create_time_series_dataset(\n",
    "        INCOME_GINI_VARIABLES, 'p0p100', COUNTRIES_TO_ANALYZE, directory)\n",
    "    \n",
    "    print(\"Creating wealth Gini coefficient dataset...\")\n",
    "    datasets['wealth_gini'] = create_time_series_dataset(\n",
    "        WEALTH_GINI_VARIABLES, 'p0p100', COUNTRIES_TO_ANALYZE, directory)\n",
    "    \n",
    "    # 4. Average income and wealth metrics (for development level comparison)\n",
    "    print(\"Creating average income dataset...\")\n",
    "    datasets['average_income'] = create_time_series_dataset(\n",
    "        INCOME_AVERAGE_VARIABLES, 'p0p100', COUNTRIES_TO_ANALYZE, directory)\n",
    "    \n",
    "    print(\"Creating average wealth dataset...\")\n",
    "    datasets['average_wealth'] = create_time_series_dataset(\n",
    "        WEALTH_AVERAGE_VARIABLES, 'p0p100', COUNTRIES_TO_ANALYZE, directory)\n",
    "    \n",
    "    # 5. Cross-sectional dataset with the latest data for all metrics\n",
    "    print(\"Creating cross-sectional dataset...\")\n",
    "    datasets['cross_section'] = create_cross_sectional_dataset(\n",
    "        COUNTRIES_TO_ANALYZE, 2020, directory)\n",
    "    \n",
    "    # 6. Calculate wealth-to-income inequality ratios for cross-sectional analysis\n",
    "    if datasets['cross_section'] is not None and not datasets['cross_section'].empty:\n",
    "        print(\"Calculating wealth-to-income inequality ratios...\")\n",
    "        calculate_wealth_income_ratios(datasets['cross_section'])\n",
    "    \n",
    "    # 7. Calculate changes in inequality metrics over time\n",
    "    print(\"Creating inequality change datasets...\")\n",
    "    for metric_type in ['income_share', 'wealth_share', 'income_gini', 'wealth_gini']:\n",
    "        percentile = 'p99p100' if 'share' in metric_type else 'p0p100'\n",
    "        variable_list = INCOME_SHARE_VARIABLES if metric_type == 'income_share' else \\\n",
    "                       WEALTH_SHARE_VARIABLES if metric_type == 'wealth_share' else \\\n",
    "                       INCOME_GINI_VARIABLES if metric_type == 'income_gini' else \\\n",
    "                       WEALTH_GINI_VARIABLES\n",
    "        \n",
    "        datasets[f'{metric_type}_change'] = create_inequality_change_dataset(\n",
    "            COUNTRIES_TO_ANALYZE, variable_list, percentile, 1980, 2020, directory)\n",
    "    \n",
    "    # 8. Development level dataset (using average income as proxy)\n",
    "    print(\"Creating development level dataset...\")\n",
    "    datasets['development_level'] = create_time_series_dataset(\n",
    "        INCOME_AVERAGE_VARIABLES, 'p0p100', COUNTRIES_TO_ANALYZE, directory)\n",
    "    \n",
    "    # Save datasets to CSV files\n",
    "    output_dir = 'output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        if df is not None and not df.empty:\n",
    "            df.to_csv(os.path.join(output_dir, f'{name}.csv'), index=False)\n",
    "            print(f\"Saved {name}.csv\")\n",
    "    \n",
    "    print(\"Dataset preparation complete!\")\n",
    "    return datasets\n",
    "\n",
    "# Function to calculate wealth-to-income inequality ratios\n",
    "def calculate_wealth_income_ratios(cross_section_df):\n",
    "    \"\"\"\n",
    "    Calculate wealth-to-income inequality ratios for cross-sectional analysis.\n",
    "    Adds these ratios directly to the dataframe.\n",
    "    \n",
    "    Args:\n",
    "        cross_section_df (pd.DataFrame): Cross-sectional dataset with inequality metrics\n",
    "    \n",
    "    Returns:\n",
    "        None (modifies dataframe in-place)\n",
    "    \"\"\"\n",
    "    # Find wealth and income share columns for top percentiles\n",
    "    wealth_cols = [col for col in cross_section_df.columns \n",
    "                  if col.endswith('_value') and 'wealth' in col and any(p in col for p in TOP_PERCENTILES)]\n",
    "    \n",
    "    income_cols = [col for col in cross_section_df.columns \n",
    "                  if col.endswith('_value') and 'income' in col and 'share' in col \n",
    "                  and any(p in col for p in TOP_PERCENTILES)]\n",
    "    \n",
    "    # Find Gini coefficient columns\n",
    "    wealth_gini_cols = [col for col in cross_section_df.columns \n",
    "                       if col.endswith('_value') and 'wealth' in col and 'gini' in col]\n",
    "    \n",
    "    income_gini_cols = [col for col in cross_section_df.columns \n",
    "                       if col.endswith('_value') and 'income' in col and 'gini' in col]\n",
    "    \n",
    "    # Calculate wealth-to-income share ratios for each percentile\n",
    "    for w_col in wealth_cols:\n",
    "        for i_col in income_cols:\n",
    "            # Make sure we're comparing the same percentile\n",
    "            w_percentile = next((p for p in TOP_PERCENTILES if p in w_col), None)\n",
    "            i_percentile = next((p for p in TOP_PERCENTILES if p in i_col), None)\n",
    "            \n",
    "            if w_percentile == i_percentile:\n",
    "                ratio_name = f\"wealth_to_income_ratio_{w_percentile}\"\n",
    "                cross_section_df[ratio_name] = cross_section_df[w_col] / cross_section_df[i_col]\n",
    "                print(f\"  Calculated {ratio_name}\")\n",
    "    \n",
    "    # Calculate wealth-to-income Gini ratio if available\n",
    "    if wealth_gini_cols and income_gini_cols:\n",
    "        cross_section_df['wealth_to_income_gini_ratio'] = \\\n",
    "            cross_section_df[wealth_gini_cols[0]] / cross_section_df[income_gini_cols[0]]\n",
    "        print(\"  Calculated wealth-to-income Gini ratio\")\n",
    "    \n",
    "    # Group countries by development level (using average income as proxy)\n",
    "    income_avg_cols = [col for col in cross_section_df.columns \n",
    "                      if col.endswith('_value') and 'income' in col and 'average' in col]\n",
    "    \n",
    "    if income_avg_cols:\n",
    "        try:\n",
    "            # Create development level categories using income levels\n",
    "            income_col = income_avg_cols[0]\n",
    "            \n",
    "            # Remove missing values for ranking\n",
    "            valid_income = cross_section_df.dropna(subset=[income_col])\n",
    "            \n",
    "            if len(valid_income) >= 6:  # Need at least 6 countries for 3 groups of 2\n",
    "                # Create ranks, handling ties\n",
    "                ranks = valid_income[income_col].rank(method='first')\n",
    "                \n",
    "                # Create quantiles with 3 groups if enough countries\n",
    "                n_groups = min(3, len(valid_income) // 2)\n",
    "                \n",
    "                development_labels = [f'Low Income', f'Middle Income', f'High Income'][:n_groups]\n",
    "                \n",
    "                # Create development level categories\n",
    "                valid_income['development_level'] = pd.qcut(\n",
    "                    ranks, q=n_groups, labels=development_labels\n",
    "                )\n",
    "                \n",
    "                # Merge back to original dataframe\n",
    "                development_mapping = dict(zip(\n",
    "                    valid_income['country_code'], \n",
    "                    valid_income['development_level']\n",
    "                ))\n",
    "                \n",
    "                # Apply mapping to original dataframe\n",
    "                cross_section_df['development_level'] = \\\n",
    "                    cross_section_df['country_code'].map(development_mapping)\n",
    "                \n",
    "                print(\"  Added development level categories\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error creating development levels: {e}\")\n",
    "    \n",
    "    # Add region groupings based on country codes\n",
    "    region_mapping = {\n",
    "        'US': 'North America',\n",
    "        'FR': 'Western Europe', \n",
    "        'DE': 'Western Europe',\n",
    "        'GB': 'Western Europe',\n",
    "        'JP': 'East Asia',\n",
    "        'BR': 'Latin America',\n",
    "        'CN': 'East Asia',\n",
    "        'RU': 'Eastern Europe',\n",
    "        'ZA': 'Africa',\n",
    "        'IN': 'South Asia',\n",
    "        'ID': 'Southeast Asia',\n",
    "        'NG': 'Africa',\n",
    "        'EG': 'Middle East & North Africa'\n",
    "    }\n",
    "    \n",
    "    cross_section_df['region'] = cross_section_df['country_code'].map(region_mapping)\n",
    "    print(\"  Added region classifications\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_data = prepare_all_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_adult_national_income = pd.read_csv(\"WID_data/WID_Per_Adult_National_Income.csv\", header=1, sep=';')\n",
    "wealth_inequality = pd.read_csv(\"WID_data/WID_Wealth_Inequality.csv\", header=1, sep=';')\n",
    "income_inequality = pd.read_csv(\"WID_data/WID_Income_Inequality.csv\", header=1, sep=';')\n",
    "national_wealth = pd.read_csv(\"WID_data/WID_National_Wealth.csv\", header=1, sep=';')\n",
    "national_wealth_to_income_ratio = pd.read_csv(\"WID_data/WID_National_Wealth_to_Net_National_Income_Ratio.csv\", header=1, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DISS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
