{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring WID dataset structure...\n",
      "Total files: 802\n",
      "Country files: 400\n",
      "Example countries: ['AD', 'AE', 'AF', 'AG', 'AI', 'AL', 'AM', 'AN', 'AO', 'AR']\n",
      "\n",
      "Total countries/regions: 400\n",
      "\n",
      "World regions:\n",
      "  Africa: 58 entries\n",
      "  Europe: 54 entries\n",
      "  Asia: 54 entries\n",
      "  Americas: 53 entries\n",
      "  Oceania: 23 entries\n",
      "\n",
      "Sample data for US:\n",
      "  Rows: 633484\n",
      "  Unique variables: 1560\n",
      "  Year range: 1800 - 2023\n",
      "\n",
      "Sample variable descriptions:\n",
      "  accmhni992: nan (USD)\n",
      "  accmhni999: nan (USD)\n",
      "  accmhoi992: nan (USD)\n",
      "  accmhoi999: nan (USD)\n",
      "  accshni992: nan (USD)\n",
      "\n",
      "Exploration complete!\n"
     ]
    }
   ],
   "source": [
    "# World Inequality Database Analysis\n",
    "# CS 328 Writing Assignment - 2025\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Create output directories if they don't exist\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Function to read WID CSV files with proper parameters\n",
    "def read_wid_csv(file_path):\n",
    "    \"\"\"\n",
    "    Read WID CSV files using the semicolon separator as specified in documentation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, sep=';', encoding='utf-8')\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to list available files in the WID data directory\n",
    "def list_wid_files(directory='wid_all_data'):\n",
    "    \"\"\"List and categorize files in the WID data directory.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"Directory {directory} not found\")\n",
    "        return None\n",
    "    \n",
    "    all_files = os.listdir(directory)\n",
    "    \n",
    "    # Categorize files\n",
    "    country_file = [f for f in all_files if f == 'WID_countries.csv']\n",
    "    data_files = sorted([f for f in all_files if f.startswith('WID_data_')])\n",
    "    metadata_files = sorted([f for f in all_files if f.startswith('WID_metadata_')])\n",
    "    other_files = [f for f in all_files if f not in country_file + data_files + metadata_files]\n",
    "    \n",
    "    # Create a summary dictionary\n",
    "    file_summary = {\n",
    "        'country_file': country_file,\n",
    "        'data_files': data_files,\n",
    "        'metadata_files': metadata_files,\n",
    "        'other_files': other_files,\n",
    "        'total_files': len(all_files),\n",
    "        'total_countries': len(data_files)\n",
    "    }\n",
    "    \n",
    "    return file_summary\n",
    "\n",
    "# Explore available countries and their metadata\n",
    "def explore_countries(directory='wid_all_data'):\n",
    "    \"\"\"Load and explore country data from WID_countries.csv.\"\"\"\n",
    "    countries_path = os.path.join(directory, 'WID_countries.csv')\n",
    "    \n",
    "    if not os.path.exists(countries_path):\n",
    "        print(f\"Country file not found at {countries_path}\")\n",
    "        return None\n",
    "    \n",
    "    countries_df = read_wid_csv(countries_path)\n",
    "    \n",
    "    if countries_df is not None:\n",
    "        # Create a summary of regions\n",
    "        region_counts = countries_df['region'].value_counts()\n",
    "        region2_counts = countries_df['region2'].value_counts()\n",
    "        \n",
    "        # Filter actual countries (2-letter codes) from regions/aggregates\n",
    "        countries_only = countries_df[countries_df['alpha2'].str.len() == 2]\n",
    "        \n",
    "        # Create a country summary\n",
    "        country_summary = {\n",
    "            'total_entries': len(countries_df),\n",
    "            'country_count': len(countries_only),\n",
    "            'regions': region_counts.to_dict(),\n",
    "            'subregions': region2_counts.to_dict()\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'countries_df': countries_df,\n",
    "            'summary': country_summary\n",
    "        }\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Function to explore the structure of a single country's data file\n",
    "def explore_country_data(country_code, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Explore the data structure for a single country.\n",
    "    \n",
    "    Args:\n",
    "        country_code (str): Two-letter country code (e.g., 'US', 'FR')\n",
    "        directory (str): Path to the WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        dict: Summary information about the country's data\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(directory, f'WID_data_{country_code}.csv')\n",
    "    metadata_path = os.path.join(directory, f'WID_metadata_{country_code}.csv')\n",
    "    \n",
    "    if not os.path.exists(data_path) or not os.path.exists(metadata_path):\n",
    "        print(f\"Data or metadata file for {country_code} not found\")\n",
    "        return None\n",
    "    \n",
    "    # Load data and metadata\n",
    "    data_df = read_wid_csv(data_path)\n",
    "    metadata_df = read_wid_csv(metadata_path)\n",
    "    \n",
    "    if data_df is None or metadata_df is None:\n",
    "        return None\n",
    "    \n",
    "    # Create data summary\n",
    "    data_summary = {\n",
    "        'rows': len(data_df),\n",
    "        'variables': data_df['variable'].nunique(),\n",
    "        'variable_list': sorted(data_df['variable'].unique()),\n",
    "        'percentiles': data_df['percentile'].nunique(),\n",
    "        'percentile_list': sorted(data_df['percentile'].unique()),\n",
    "        'years': {\n",
    "            'min': data_df['year'].min(),\n",
    "            'max': data_df['year'].max(),\n",
    "            'count': data_df['year'].nunique()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create metadata summary\n",
    "    metadata_summary = {\n",
    "        'rows': len(metadata_df),\n",
    "        'unique_variables': metadata_df['variable'].nunique(),\n",
    "        'variable_list': sorted(metadata_df['variable'].unique())\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'data_df': data_df,\n",
    "        'metadata_df': metadata_df,\n",
    "        'data_summary': data_summary,\n",
    "        'metadata_summary': metadata_summary\n",
    "    }\n",
    "\n",
    "# Function to extract variable descriptions from metadata\n",
    "def get_variable_descriptions(metadata_df):\n",
    "    \"\"\"\n",
    "    Extract unique variable descriptions from metadata.\n",
    "    \n",
    "    Args:\n",
    "        metadata_df (pd.DataFrame): Metadata dataframe\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataframe with variable codes and descriptions\n",
    "    \"\"\"\n",
    "    if metadata_df is None:\n",
    "        return None\n",
    "    \n",
    "    # Check if required columns exist\n",
    "    required_columns = ['variable', 'simpledes', 'technicaldes', 'unit']\n",
    "    if not all(col in metadata_df.columns for col in required_columns):\n",
    "        print(f\"Metadata is missing required columns. Available columns: {metadata_df.columns.tolist()}\")\n",
    "        return None\n",
    "    \n",
    "    # Extract unique variable descriptions\n",
    "    var_descriptions = metadata_df[required_columns].drop_duplicates()\n",
    "    \n",
    "    return var_descriptions.sort_values('variable').reset_index(drop=True)\n",
    "\n",
    "# Function to examine variable availability across countries\n",
    "def compare_variable_availability(country_list, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Compare which variables are available across multiple countries.\n",
    "    \n",
    "    Args:\n",
    "        country_list (list): List of country codes to compare\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Data frame showing variable availability by country\n",
    "    \"\"\"\n",
    "    availability_data = []\n",
    "    \n",
    "    for country in country_list:\n",
    "        data_path = os.path.join(directory, f'WID_data_{country}.csv')\n",
    "        \n",
    "        if os.path.exists(data_path):\n",
    "            data_df = read_wid_csv(data_path)\n",
    "            \n",
    "            if data_df is not None:\n",
    "                variables = data_df['variable'].unique()\n",
    "                \n",
    "                for var in variables:\n",
    "                    # Check year range for this variable\n",
    "                    var_data = data_df[data_df['variable'] == var]\n",
    "                    year_min = var_data['year'].min()\n",
    "                    year_max = var_data['year'].max()\n",
    "                    \n",
    "                    availability_data.append({\n",
    "                        'country': country,\n",
    "                        'variable': var,\n",
    "                        'available': True,\n",
    "                        'year_min': year_min,\n",
    "                        'year_max': year_max,\n",
    "                        'year_count': var_data['year'].nunique()\n",
    "                    })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    availability_df = pd.DataFrame(availability_data)\n",
    "    \n",
    "    # Create a pivot table of availability\n",
    "    if not availability_df.empty:\n",
    "        pivot_df = pd.pivot_table(\n",
    "            availability_df, \n",
    "            values='available',\n",
    "            index='variable',\n",
    "            columns='country',\n",
    "            aggfunc=lambda x: True if len(x) > 0 else False,\n",
    "            fill_value=False\n",
    "        )\n",
    "        \n",
    "        # Add a total count column\n",
    "        pivot_df['total_countries'] = pivot_df.sum(axis=1)\n",
    "        \n",
    "        # Sort by availability\n",
    "        pivot_df = pivot_df.sort_values('total_countries', ascending=False)\n",
    "        \n",
    "        return pivot_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Main execution to explore the dataset\n",
    "def explore_dataset(directory='wid_all_data'):\n",
    "    \"\"\"Main function to explore the WID dataset structure.\"\"\"\n",
    "    print(\"Exploring WID dataset structure...\")\n",
    "    \n",
    "    # List available files\n",
    "    files = list_wid_files(directory)\n",
    "    if files:\n",
    "        print(f\"Total files: {files['total_files']}\")\n",
    "        print(f\"Country files: {len(files['data_files'])}\")\n",
    "        \n",
    "        # Show some example countries\n",
    "        if files['data_files']:\n",
    "            print(\"Example countries:\", [f.replace('WID_data_', '').replace('.csv', '') \n",
    "                                         for f in files['data_files'][:10]])\n",
    "    \n",
    "    # Explore countries metadata\n",
    "    countries_info = explore_countries(directory)\n",
    "    if countries_info:\n",
    "        countries_df = countries_info['countries_df']\n",
    "        print(f\"\\nTotal countries/regions: {len(countries_df)}\")\n",
    "        \n",
    "        # Display regions\n",
    "        print(\"\\nWorld regions:\")\n",
    "        for region, count in countries_info['summary']['regions'].items():\n",
    "            print(f\"  {region}: {count} entries\")\n",
    "    \n",
    "    # Explore a sample country\n",
    "    sample_country = 'US'  # United States as example\n",
    "    country_info = explore_country_data(sample_country, directory)\n",
    "    \n",
    "    if country_info:\n",
    "        print(f\"\\nSample data for {sample_country}:\")\n",
    "        print(f\"  Rows: {country_info['data_summary']['rows']}\")\n",
    "        print(f\"  Unique variables: {country_info['data_summary']['variables']}\")\n",
    "        print(f\"  Year range: {country_info['data_summary']['years']['min']} - {country_info['data_summary']['years']['max']}\")\n",
    "        \n",
    "        # Show some variable descriptions\n",
    "        var_desc = get_variable_descriptions(country_info['metadata_df'])\n",
    "        if var_desc is not None and len(var_desc) > 0:\n",
    "            print(\"\\nSample variable descriptions:\")\n",
    "            for _, row in var_desc.head(5).iterrows():\n",
    "                print(f\"  {row['variable']}: {row['simpledes']} ({row['unit']})\")\n",
    "    \n",
    "    return {\n",
    "        'files': files,\n",
    "        'countries_info': countries_info,\n",
    "        'sample_country_info': country_info\n",
    "    }\n",
    "\n",
    "# Run the exploration if executed as a script\n",
    "if __name__ == \"__main__\":\n",
    "    explore_result = explore_dataset()\n",
    "    print(\"\\nExploration complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_exploration'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Import functions from our exploration module\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_exploration\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m read_wid_csv, explore_countries, explore_country_data\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Key variables we'll be using based on the WID documentation\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# sptinc992j - Share (s) of pre-tax national income (ptinc) for adults (992) with equal-split (j)\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# ahweal992j - Average (a) household wealth (hweal) for adults (992) with equal-split (j)\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# anninc992i - Average (a) national income (nninc) for adults (992) for individuals (i)\u001b[39;00m\n\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Define percentiles of interest\u001b[39;00m\n\u001b[32m     23\u001b[39m TOP_PERCENTILES = [\u001b[33m'\u001b[39m\u001b[33mp99p100\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mp90p100\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mp99.9p100\u001b[39m\u001b[33m'\u001b[39m]  \u001b[38;5;66;03m# Top 1%, Top 10%, Top 0.1%\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data_exploration'"
     ]
    }
   ],
   "source": [
    "# World Inequality Database - Data Processing and Feature Engineering\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import functions from our exploration module\n",
    "from data_exploration import read_wid_csv, explore_countries, explore_country_data\n",
    "\n",
    "# Key variables we'll be using based on the WID documentation\n",
    "# sptinc992j - Share (s) of pre-tax national income (ptinc) for adults (992) with equal-split (j)\n",
    "# ahweal992j - Average (a) household wealth (hweal) for adults (992) with equal-split (j)\n",
    "# anninc992i - Average (a) national income (nninc) for adults (992) for individuals (i)\n",
    "\n",
    "# Define percentiles of interest\n",
    "TOP_PERCENTILES = ['p99p100', 'p90p100', 'p99.9p100']  # Top 1%, Top 10%, Top 0.1%\n",
    "BOTTOM_PERCENTILES = ['p0p50', 'p0p90']  # Bottom 50%, Bottom 90%\n",
    "MIDDLE_PERCENTILES = ['p50p90']  # Middle 40%\n",
    "\n",
    "# Countries to include in our analysis\n",
    "# We'll select a diverse set of countries from different regions and development levels\n",
    "COUNTRIES_TO_ANALYZE = [\n",
    "    # High-income countries\n",
    "    'US',   # United States\n",
    "    'FR',   # France\n",
    "    'DE',   # Germany\n",
    "    'GB',   # United Kingdom\n",
    "    'JP',   # Japan\n",
    "    \n",
    "    # Upper-middle income countries\n",
    "    'BR',   # Brazil\n",
    "    'CN',   # China\n",
    "    'RU',   # Russia\n",
    "    'ZA',   # South Africa\n",
    "    \n",
    "    # Lower-middle and low-income countries\n",
    "    'IN',   # India\n",
    "    'ID',   # Indonesia\n",
    "    'NG',   # Nigeria\n",
    "    'EG'    # Egypt\n",
    "]\n",
    "\n",
    "# Function to load country data with selected variables\n",
    "def load_country_data(country_code, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Load specific inequality variables for a given country.\n",
    "    \n",
    "    Args:\n",
    "        country_code (str): Two-letter country code\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (data_df, metadata_df) for the country\n",
    "    \"\"\"\n",
    "    data_path = os.path.join(directory, f'WID_data_{country_code}.csv')\n",
    "    metadata_path = os.path.join(directory, f'WID_metadata_{country_code}.csv')\n",
    "    \n",
    "    if not os.path.exists(data_path) or not os.path.exists(metadata_path):\n",
    "        print(f\"Data or metadata not found for {country_code}\")\n",
    "        return None, None\n",
    "    \n",
    "    data_df = read_wid_csv(data_path)\n",
    "    metadata_df = read_wid_csv(metadata_path)\n",
    "    \n",
    "    return data_df, metadata_df\n",
    "\n",
    "# Function to create a dataset for a specific inequality metric\n",
    "def create_inequality_dataset(countries, variable_code, percentiles, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Create a dataset comparing a specific inequality variable across countries.\n",
    "    \n",
    "    Args:\n",
    "        countries (list): List of country codes\n",
    "        variable_code (str): WID variable code (e.g., 'sptinc992j' for pre-tax income share)\n",
    "        percentiles (list): List of percentile codes (e.g., ['p99p100', 'p0p50'])\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset with inequality data\n",
    "    \"\"\"\n",
    "    # Load country information for names\n",
    "    countries_info = explore_countries(directory)\n",
    "    if countries_info is None:\n",
    "        print(\"Could not load country information\")\n",
    "        return None\n",
    "    \n",
    "    countries_df = countries_info['countries_df']\n",
    "    country_name_map = dict(zip(countries_df['alpha2'], countries_df['shortname']))\n",
    "    \n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    for country in countries:\n",
    "        data_df, metadata_df = load_country_data(country, directory)\n",
    "        \n",
    "        if data_df is None:\n",
    "            print(f\"Skipping {country} - could not load data\")\n",
    "            continue\n",
    "        \n",
    "        # Filter for the requested variable and percentiles\n",
    "        filtered_df = data_df[(data_df['variable'] == variable_code) & \n",
    "                             (data_df['percentile'].isin(percentiles))]\n",
    "        \n",
    "        if filtered_df.empty:\n",
    "            print(f\"No data for {variable_code} with percentiles {percentiles} in {country}\")\n",
    "            continue\n",
    "        \n",
    "        # Add country name\n",
    "        filtered_df['country_code'] = country\n",
    "        filtered_df['country_name'] = country_name_map.get(country, country)\n",
    "        \n",
    "        # Append to combined dataset\n",
    "        combined_df = pd.concat([combined_df, filtered_df])\n",
    "    \n",
    "    if combined_df.empty:\n",
    "        print(f\"No data found for {variable_code} across specified countries and percentiles\")\n",
    "        return None\n",
    "    \n",
    "    return combined_df\n",
    "\n",
    "# Function to create a comparative dataset of income/wealth distribution over time\n",
    "def create_time_series_dataset(variable_code, percentile, countries=COUNTRIES_TO_ANALYZE, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Create a dataset of inequality metrics over time for multiple countries.\n",
    "    \n",
    "    Args:\n",
    "        variable_code (str): WID variable code\n",
    "        percentile (str): Percentile code\n",
    "        countries (list): List of country codes\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Time series data for the specified variable and percentile\n",
    "    \"\"\"\n",
    "    # Get variable description\n",
    "    sample_country = countries[0]\n",
    "    _, metadata_df = load_country_data(sample_country, directory)\n",
    "    \n",
    "    variable_desc = None\n",
    "    if metadata_df is not None:\n",
    "        var_info = metadata_df[metadata_df['variable'] == variable_code]\n",
    "        if not var_info.empty:\n",
    "            variable_desc = var_info.iloc[0]['simpledes']\n",
    "    \n",
    "    dataset = create_inequality_dataset(countries, variable_code, [percentile], directory)\n",
    "    \n",
    "    if dataset is not None:\n",
    "        # Pivot to have years as columns and countries as rows for easier plotting\n",
    "        dataset = dataset.sort_values(['country_name', 'year'])\n",
    "        \n",
    "        # Add metadata\n",
    "        dataset.attrs['variable_code'] = variable_code\n",
    "        dataset.attrs['variable_desc'] = variable_desc\n",
    "        dataset.attrs['percentile'] = percentile\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Function to create a dataset for GDP per capita\n",
    "def create_gdp_dataset(countries=COUNTRIES_TO_ANALYZE, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Create a dataset of GDP per capita for comparison with inequality metrics.\n",
    "    Using national income per adult as a proxy.\n",
    "    \n",
    "    Args:\n",
    "        countries (list): List of country codes\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: GDP per capita data\n",
    "    \"\"\"\n",
    "    # National income per adult in constant local currency\n",
    "    variable_code = 'anninc992i'\n",
    "    \n",
    "    # We don't need a percentile for this aggregate measure, but WID still requires one\n",
    "    # p0p100 represents the entire population\n",
    "    gdp_data = create_inequality_dataset(countries, variable_code, ['p0p100'], directory)\n",
    "    \n",
    "    if gdp_data is not None:\n",
    "        # Add variable description\n",
    "        gdp_data.attrs['variable_desc'] = 'National Income per Adult'\n",
    "        \n",
    "        # Convert to common currency (USD) using most recent PPP rates\n",
    "        # This would require additional implementation to get PPP conversion rates\n",
    "        # For simplicity, we'll leave the values in local currency\n",
    "    \n",
    "    return gdp_data\n",
    "\n",
    "# Function to combine multiple inequality metrics for cross-sectional analysis\n",
    "def create_cross_sectional_dataset(countries=COUNTRIES_TO_ANALYZE, year=2020, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Create a cross-sectional dataset combining multiple inequality metrics for a specific year.\n",
    "    \n",
    "    Args:\n",
    "        countries (list): List of country codes\n",
    "        year (int): Reference year for the cross-section\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined dataset with multiple inequality metrics\n",
    "    \"\"\"\n",
    "    # Define the variables and percentiles we want to include\n",
    "    metrics = [\n",
    "        {'variable': 'sptinc992j', 'percentile': 'p99p100', 'name': 'top1_income_share'},\n",
    "        {'variable': 'sptinc992j', 'percentile': 'p90p100', 'name': 'top10_income_share'},\n",
    "        {'variable': 'sptinc992j', 'percentile': 'p0p50', 'name': 'bottom50_income_share'},\n",
    "        {'variable': 'sptinc992j', 'percentile': 'p50p90', 'name': 'middle40_income_share'},\n",
    "        {'variable': 'shweal992j', 'percentile': 'p99p100', 'name': 'top1_wealth_share'},\n",
    "        {'variable': 'shweal992j', 'percentile': 'p90p100', 'name': 'top10_wealth_share'},\n",
    "        {'variable': 'shweal992j', 'percentile': 'p0p50', 'name': 'bottom50_wealth_share'},\n",
    "        {'variable': 'anninc992i', 'percentile': 'p0p100', 'name': 'gdp_per_adult'},\n",
    "        # Add the Gini coefficient if available\n",
    "        {'variable': 'gptinc992i', 'percentile': 'p0p100', 'name': 'income_gini'}\n",
    "    ]\n",
    "    \n",
    "    # Initialize results dataframe\n",
    "    result_data = []\n",
    "    \n",
    "    # Load country info for names\n",
    "    countries_info = explore_countries(directory)\n",
    "    countries_df = countries_info['countries_df'] if countries_info else None\n",
    "    country_name_map = dict(zip(countries_df['alpha2'], countries_df['shortname'])) if countries_df is not None else {}\n",
    "    \n",
    "    # Process each country\n",
    "    for country in countries:\n",
    "        country_data = {'country_code': country, 'country_name': country_name_map.get(country, country)}\n",
    "        \n",
    "        data_df, _ = load_country_data(country, directory)\n",
    "        \n",
    "        if data_df is None:\n",
    "            continue\n",
    "        \n",
    "        # Extract values for each metric\n",
    "        for metric in metrics:\n",
    "            var = metric['variable']\n",
    "            perc = metric['percentile']\n",
    "            name = metric['name']\n",
    "            \n",
    "            # Filter data for this variable, percentile, and closest year\n",
    "            filtered = data_df[(data_df['variable'] == var) & (data_df['percentile'] == perc)]\n",
    "            \n",
    "            if filtered.empty:\n",
    "                country_data[name] = np.nan\n",
    "                continue\n",
    "            \n",
    "            # Find closest year to the reference year\n",
    "            available_years = filtered['year'].unique()\n",
    "            if year in available_years:\n",
    "                closest_year = year\n",
    "            else:\n",
    "                closest_year = available_years[np.abs(available_years - year).argmin()]\n",
    "            \n",
    "            # Get the value for the closest year\n",
    "            year_value = filtered[filtered['year'] == closest_year]['value'].iloc[0]\n",
    "            country_data[name] = year_value\n",
    "            country_data[f'{name}_year'] = closest_year\n",
    "        \n",
    "        # Add region information if available\n",
    "        if countries_df is not None:\n",
    "            country_region = countries_df[countries_df['alpha2'] == country]\n",
    "            if not country_region.empty:\n",
    "                country_data['region'] = country_region['region'].iloc[0]\n",
    "                country_data['region2'] = country_region['region2'].iloc[0]\n",
    "        \n",
    "        result_data.append(country_data)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Function to create a dataset comparing changes in inequality over time\n",
    "def create_inequality_change_dataset(countries=COUNTRIES_TO_ANALYZE, \n",
    "                                     variable_code='sptinc992j',\n",
    "                                     percentile='p99p100', \n",
    "                                     start_year=1980, \n",
    "                                     end_year=2020,\n",
    "                                     directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Create a dataset showing changes in inequality metrics over time.\n",
    "    \n",
    "    Args:\n",
    "        countries (list): List of country codes\n",
    "        variable_code (str): WID variable code\n",
    "        percentile (str): Percentile code\n",
    "        start_year (int): Starting year for change calculation\n",
    "        end_year (int): Ending year for change calculation\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with inequality changes\n",
    "    \"\"\"\n",
    "    # Get the time series data\n",
    "    time_series = create_time_series_dataset(variable_code, percentile, countries, directory)\n",
    "    \n",
    "    if time_series is None:\n",
    "        return None\n",
    "    \n",
    "    # Calculate changes\n",
    "    change_data = []\n",
    "    \n",
    "    # Group by country\n",
    "    for country, group in time_series.groupby('country_code'):\n",
    "        group = group.sort_values('year')\n",
    "        country_name = group['country_name'].iloc[0]\n",
    "        \n",
    "        # Try to get values for exact years\n",
    "        start_data = group[group['year'] == start_year]\n",
    "        end_data = group[group['year'] == end_year]\n",
    "        \n",
    "        # If exact years not available, find closest years\n",
    "        if start_data.empty:\n",
    "            available_years = group['year'].unique()\n",
    "            closest_start = available_years[np.abs(available_years - start_year).argmin()]\n",
    "            start_data = group[group['year'] == closest_start]\n",
    "        \n",
    "        if end_data.empty:\n",
    "            available_years = group['year'].unique()\n",
    "            closest_end = available_years[np.abs(available_years - end_year).argmin()]\n",
    "            end_data = group[group['year'] == closest_end]\n",
    "        \n",
    "        # Skip if we don't have data for both periods\n",
    "        if start_data.empty or end_data.empty:\n",
    "            print(f\"Insufficient data for {country} to calculate changes\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate changes\n",
    "        start_value = start_data['value'].iloc[0]\n",
    "        end_value = end_data['value'].iloc[0]\n",
    "        actual_start_year = start_data['year'].iloc[0]\n",
    "        actual_end_year = end_data['year'].iloc[0]\n",
    "        \n",
    "        absolute_change = end_value - start_value\n",
    "        percent_change = (absolute_change / start_value) * 100 if start_value != 0 else np.nan\n",
    "        \n",
    "        change_data.append({\n",
    "            'country_code': country,\n",
    "            'country_name': country_name,\n",
    "            'start_year': actual_start_year,\n",
    "            'end_year': actual_end_year,\n",
    "            'start_value': start_value,\n",
    "            'end_value': end_value,\n",
    "            'absolute_change': absolute_change,\n",
    "            'percent_change': percent_change\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    change_df = pd.DataFrame(change_data)\n",
    "    \n",
    "    # Add metadata\n",
    "    change_df.attrs['variable_code'] = variable_code\n",
    "    change_df.attrs['variable_desc'] = time_series.attrs.get('variable_desc', '')\n",
    "    change_df.attrs['percentile'] = percentile\n",
    "    \n",
    "    return change_df\n",
    "\n",
    "# Function to combine income and wealth inequality data for correlation analysis\n",
    "def create_correlation_dataset(countries=COUNTRIES_TO_ANALYZE, reference_year=2020, directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Create a dataset to analyze correlations between income and wealth inequality.\n",
    "    \n",
    "    Args:\n",
    "        countries (list): List of country codes\n",
    "        reference_year (int): Reference year for the cross-section\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with income and wealth inequality metrics\n",
    "    \"\"\"\n",
    "    # Get cross-sectional data\n",
    "    cross_section = create_cross_sectional_dataset(countries, reference_year, directory)\n",
    "    \n",
    "    if cross_section is None or cross_section.empty:\n",
    "        print(\"Could not create cross-sectional dataset for correlation analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Create metrics for correlation analysis\n",
    "    corr_metrics = [\n",
    "        ('top1_income_share', 'top1_wealth_share'),\n",
    "        ('top10_income_share', 'top10_wealth_share'),\n",
    "        ('bottom50_income_share', 'bottom50_wealth_share'),\n",
    "        ('gdp_per_adult', 'top1_income_share'),\n",
    "        ('gdp_per_adult', 'top1_wealth_share')\n",
    "    ]\n",
    "    \n",
    "    # Calculate correlations\n",
    "    correlations = {}\n",
    "    \n",
    "    for x_var, y_var in corr_metrics:\n",
    "        if x_var in cross_section.columns and y_var in cross_section.columns:\n",
    "            # Filter out NaN values\n",
    "            valid_data = cross_section[[x_var, y_var]].dropna()\n",
    "            \n",
    "            if len(valid_data) >= 5:  # Require at least 5 countries for meaningful correlation\n",
    "                corr, p_value = stats.pearsonr(valid_data[x_var], valid_data[y_var])\n",
    "                correlations[f'{x_var}_vs_{y_var}'] = {\n",
    "                    'correlation': corr,\n",
    "                    'p_value': p_value,\n",
    "                    'n': len(valid_data)\n",
    "                }\n",
    "    \n",
    "    # Add correlations to dataset attributes\n",
    "    cross_section.attrs['correlations'] = correlations\n",
    "    \n",
    "    return cross_section\n",
    "\n",
    "# Main function to prepare all datasets\n",
    "def prepare_all_datasets(directory='wid_all_data'):\n",
    "    \"\"\"\n",
    "    Prepare all datasets needed for our inequality analysis.\n",
    "    \n",
    "    Args:\n",
    "        directory (str): Path to WID data directory\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of prepared datasets\n",
    "    \"\"\"\n",
    "    print(\"Preparing inequality datasets...\")\n",
    "    \n",
    "    datasets = {}\n",
    "    \n",
    "    # 1. Time series of top 1% income share\n",
    "    print(\"Creating top 1% income share time series...\")\n",
    "    datasets['top1_income_time'] = create_time_series_dataset(\n",
    "        'sptinc992j', 'p99p100', COUNTRIES_TO_ANALYZE, directory)\n",
    "    \n",
    "    # 2. Time series of top 10% income share \n",
    "    print(\"Creating top 10% income share time series...\")\n",
    "    datasets['top10_income_time'] = create_time_series_dataset(\n",
    "        'sptinc992j', 'p90p100', COUNTRIES_TO_ANALYZE, directory)\n",
    "    \n",
    "    # 3. Time series of bottom 50% income share\n",
    "    print(\"Creating bottom 50% income share time series...\")\n",
    "    datasets['bottom50_income_time'] = create_time_series_dataset(\n",
    "        'sptinc992j', 'p0p50', COUNTRIES_TO_ANALYZE, directory)\n",
    "    \n",
    "    # 4. Time series of top 1% wealth share\n",
    "    print(\"Creating top 1% wealth share time series...\")\n",
    "    datasets['top1_wealth_time'] = create_time_series_dataset(\n",
    "        'shweal992j', 'p99p100', COUNTRIES_TO_ANALYZE, directory)\n",
    "    \n",
    "    # 5. Cross-sectional data for most recent year\n",
    "    print(\"Creating cross-sectional dataset...\")\n",
    "    datasets['cross_section'] = create_cross_sectional_dataset(\n",
    "        COUNTRIES_TO_ANALYZE, 2020, directory)\n",
    "    \n",
    "    # 6. Changes in income inequality (1980-2020)\n",
    "    print(\"Creating income inequality change dataset...\")\n",
    "    datasets['income_change'] = create_inequality_change_dataset(\n",
    "        COUNTRIES_TO_ANALYZE, 'sptinc992j', 'p99p100', 1980, 2020, directory)\n",
    "    \n",
    "    # 7. Changes in wealth inequality (1980-2020)\n",
    "    print(\"Creating wealth inequality change dataset...\")\n",
    "    datasets['wealth_change'] = create_inequality_change_dataset(\n",
    "        COUNTRIES_TO_ANALYZE, 'shweal992j', 'p99p100', 1980, 2020, directory)\n",
    "    \n",
    "    # 8. Correlation dataset\n",
    "    print(\"Creating correlation dataset...\")\n",
    "    datasets['correlation'] = create_correlation_dataset(\n",
    "        COUNTRIES_TO_ANALYZE, 2020, directory)\n",
    "    \n",
    "    # Save datasets to CSV files\n",
    "    output_dir = 'output'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for name, df in datasets.items():\n",
    "        if df is not None:\n",
    "            df.to_csv(os.path.join(output_dir, f'{name}.csv'), index=False)\n",
    "            print(f\"Saved {name}.csv\")\n",
    "    \n",
    "    print(\"Dataset preparation complete!\")\n",
    "    return datasets\n",
    "\n",
    "# Run the data preparation if executed as a script\n",
    "if __name__ == \"__main__\":\n",
    "    prepared_data = prepare_all_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DISS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
